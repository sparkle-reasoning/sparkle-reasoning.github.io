<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SPARKLE studies how reinforcement learning impacts mathematical reasoning in LLMs—including plan-following, knowledge use, subproblem decomposition, and the role of problem difficulty.">
  <meta property="og:title" content="SPARKLE: Dissecting Mathematical Reasoning for LLMs Under RL"/>
  <meta property="og:description" content="SPARKLE studies how reinforcement learning impacts mathematical reasoning in LLMs—including plan-following, knowledge use, subproblem decomposition, and the role of problem difficulty."/>
  <meta property="og:url" content="https://sparkle-reasoning.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/sparkle_icon1.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="reinforcement learning, large language models, LLM reasoning, mathematical reasoning, benchmark, subproblem decomposition, plan-following, knowledge integration, SPARKLE framework">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SPARKLE</title>
  <link rel="icon" type="image/x-icon" href="static/images/sparkle_icon1.png">
  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet"> -->

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="static/images/sparkle_icon1.png" alt="Sparkle Icon" style="height: 4.5rem; vertical-align: middle; margin-right: 0.5rem;">Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                  <span class="author-block">
                    <a href="https://jiayuww.github.io/">Jiayu Wang</a><sup style="color:#ed4b82;">1</sup>,</span>
                  <span class="author-block">
                    <a href="https://alvinmingsf.github.io/">Yifei Ming</a><sup style="color:#6fbf73;">2</sup>,</span>
                  <span class="author-block">
                    <a href="https://vincent950129.github.io/">Zixuan Ke</a><sup style="color:#6fbf73;">2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="http://cmxiong.com/">Caiming Xiong</a><sup style="color:#6fbf73">2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://raihanjoty.github.io/">Shafiq Joty</a><sup style="color:#6fbf73">2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://pages.cs.wisc.edu/~aws/">Aws Albarghouthi</a><sup style="color:#ed4b82">1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://pages.cs.wisc.edu/~fredsala/">Frederic Sala</a><sup style="color:#ed4b82">1</sup>
                  </span>
                </div>
      
                <div class="is-size-5 publication-authors">
                  <span class="author-block"><sup style="color:#ed4b82">1</sup>University of Wisconsin-Madison,</span>
                  <span class="author-block"><sup style="color:#6fbf73">2</sup>Salesforce AI Research</span>
                </div>

                <div class="column has-text-centered">
                  <div class="publication-links">
                        <!-- Arxiv PDF link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2506.04723" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>arXiv Paper</span>
                    </a>
                  </span>

                  <!-- Model & Dataset Link -->
                  <span class="link-block">
                    <a href="https://huggingface.co/sparkle-reasoning"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <p style="font-size:18px">🤗</p>
                      </span>
                      <span>Model & Dataset</span>
                    </a>
                  </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Image Section -->
<div class="container has-text-centered">
  <div class="columns is-centered">
      <div class="column is-8">
          <!-- Image -->
          <div class="publication-image">
              <img src="static/images/ba_teaser_full_v3.png" alt="SPARKLE Framework Overview">
          </div>
          
          <!-- Caption/Comments -->
          <div class="content publication-caption">
              <p class="has-text-grey-dark is-size-6" style="text-align: justify;">
                <b>SPARKLE</b> is a fine-grained framework for evaluating LLM reasoning improvements under RL, analyzing models along three key axes:
                <b>plan-following and execution</b>, <b>knowledge utilization</b>, and <b>subproblem decomposition</b>. 
                The benchmark includes annotated planning skeletons, curated knowledge, and decomposed subproblems to systematically study these capabilities and the impact of problem difficulty.
              </p>
          </div>
      </div>
  </div>
</div>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement learning (RL) has become the dominant paradigm for endowing language models with advanced reasoning capabilities. Despite the substantial empirical gains demonstrated by RL-based training methods like GRPO, a granular understanding of their advantages is still lacking. To address this gap, we introduce a fine-grained analytic framework to dissect the impact of RL on reasoning. Our framework specifically investigates key elements that have been hypothesized to benefit from RL training: (1) plan-following and execution, (2) problem decomposition, and (3) improved reasoning and knowledge utilization. Using this framework, we gain insights beyond mere accuracy. For instance, providing models with explicit step-by-step plans surprisingly degrades performance on the most challenging benchmarks, yet RL-tuned models exhibit greater robustness, experiencing markedly smaller performance drops than their base counterparts. This suggests that RL may not primarily enhance the execution of external plans but rather empower models to formulate and follow internal strategies better suited to their reasoning processes. Conversely, we observe that RL enhances the model's capacity to integrate provided knowledge into its reasoning process, leading to performance improvements across diverse tasks. We also study difficulty, showing improved training by developing new ways to exploit hard problems. Our findings lay a foundation for more principled training and evaluation of reasoning models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Main Results Section Header -->
<div class="container has-text-centered">
  <div class="columns is-centered">
      <div class="column is-8">
          <div class="section-title" style="margin-top: 4rem;">
              <h2 class="title is-3">Curriculum-style RL Training</h2>
          </div>
      </div>
  </div>
</div>

<!-- Main Results Section Header -->
<div class="container has-text-centered">
  <div class="columns is-centered">
    <div class="column is-8">

      <div class="has-text-left">
        <p>
          A common belief is that hard problems are not useful for reinforcement learning (RL) because they provide no reward signals. 
          However, our findings show that <strong>hard problems can still be effective for RL training when partially solved steps are included</strong>—a curriculum-style approach that scaffolds learning.
          Specifically, we explore two-stage curriculum-style RL training. In the first stage, the model is trained on the full dataset to build a strong foundation. In the second stage, we explore three data mixing strategies and find that <strong>continuing training on the hardest problems—augmented with partial solution steps—leads to the best performance</strong>. This method consistently outperforms training solely on hard problems without augmentation, or using a mix of easy and hard problems, especially for improving performance on the most challenging tasks.
        </p>
      </div>

      <!-- Original vs Augmented Prompt Example -->
      <div class="box has-text-left" style="background-color: #f9f9f9; margin-top: 2rem;">
        <h4 class="title is-5" style="color: #363636;">🧠 Example: Original vs. Augmented Prompt</h4>

        <div class="content">
          <p><strong>🔵 Original prompt:</strong></p>
          <blockquote style="border-left: 4px solid #ccc; padding-left: 1rem; margin-left: 0; font-size: 1.05rem;">
            One of Euler's conjectures was disproved in the 1960s by three American mathematicians when they showed there was a positive integer such that:<br>
            <span style="font-family: 'Courier New', Courier, monospace;">133⁵ + 110⁵ + 84⁵ + 27⁵ = n⁵</span><br>
            Find the value of <em>n</em>.
          </blockquote>

          <p><strong>🎯 Original question augmented with partial solution:</strong></p>
          <blockquote style="border-left: 4px solid #ccc; padding-left: 1rem; margin-left: 0; font-size: 1.05rem;">
            One of Euler's conjectures was disproved in the 1960s by three American mathematicians when they showed there was a positive integer such that:<br>
            <span style="font-family: 'Courier New', Courier, monospace;">133⁵ + 110⁵ + 84⁵ + 27⁵ = n⁵</span><br>
            Find the value of <em>n</em>.<br><br>

            <span style="color: #1f77b4;">
              Taking the given equation modulo 2, 3, and 5, respectively, we have:<br>
              <span style="font-family: 'Courier New', Courier, monospace;">n⁵ ≡ 0 (mod 2), n⁵ ≡ 0 (mod 3), n⁵ ≡ 4 (mod 5)</span>
            </span>
          </blockquote>
        </div>
      </div>

    </div>
  </div>
</div>

<!-- Main Results Section Header -->
<div class="container has-text-centered">
  <div class="columns is-centered">
      <div class="column is-8">
          <div class="section-title" style="margin-top: 4rem;">
              <h2 class="title is-3">Key Takeaways</h2>
          </div>
      </div>
  </div>
</div>

<!-- Insight Blocks -->
<section class="section">
  <div class="container is-max-desktop">

    <!-- Insight 1: Hard Problems -->
    <div class="box">
      <h3 class="subtitle is-4" style="color: #8B0000;">
        💡 <b>Insight 1: Hard problems remain valuable when appropriately structured by augmenting problems with partial solutions during RL training.</b>
      </h3>
      <div class="table-container" style="margin-top: 1rem;">
        <table class="table is-striped is-hoverable is-fullwidth">
          <thead>
            <tr style="background-color: #e6f2ff;">
              <th>Model</th><th>AIME</th><th>AMC</th><th>MATH500</th><th>GSM8K</th><th>Olympiad</th><th>Avg.</th>
            </tr>
          </thead>
          <tbody>
            <tr><td>Qwen-2.5-Math-7B-Base</td><td>16.67</td><td>42.50</td><td>44.03</td><td>42.53</td><td>28.65</td><td>35.23</td></tr>
            <tr><td>SparkleRL-Stage 1</td><td>46.67 <span class="has-text-danger">(↑30.00)</span></td><td>67.50 <span class="has-text-danger">(↑25.00)</span></td><td>80.00 <span class="has-text-danger">(↑35.97)</span></td><td>91.77 <span class="has-text-danger">(↑49.24)</span></td><td>39.11 <span class="has-text-danger">(↑10.46)</span></td><td>65.01</td></tr>
            <tr><td>SparkleRL-Stage 2 (Hard)</td><td>41.67 <span class="has-text-danger">(↑25.00)</span></td><td>65.94 <span class="has-text-danger">(↑23.44)</span></td><td>80.50 <span class="has-text-danger">(↑36.47)</span></td><td>92.45 <span class="has-text-danger">(↑49.92)</span></td><td>37.39 <span class="has-text-danger">(↑8.74)</span></td><td>63.59</td></tr>
            <tr><td>SparkleRL-Stage 2 (Mix)</td><td>40.00 <span class="has-text-danger">(↑23.33)</span></td><td>63.44 <span class="has-text-danger">(↑20.94)</span></td><td>80.78 <span class="has-text-danger">(↑36.75)</span></td><td><strong>92.52</strong> <span class="has-text-danger">(↑49.99)</span></td><td>38.85 <span class="has-text-danger">(↑10.20)</span></td><td>63.12</td></tr>
            <tr class="has-background-light"><td><strong>SparkleRL-Stage 2 (Aug)</strong></td><td><strong>50.42</strong> <span class="has-text-danger">(↑33.75)</span></td><td><strong>71.25</strong> <span class="has-text-danger">(↑28.75)</span></td><td><strong>81.00</strong> <span class="has-text-danger">(↑36.97)</span></td><td>92.38 <span class="has-text-danger">(↑49.85)</span></td><td><strong>40.11</strong> <span class="has-text-danger">(↑11.46)</span></td><td><strong>67.03</strong></td></tr>
          </tbody>
        </table>
        <p class="has-text-grey-dark is-size-6"><strong>Table:</strong> Avg@8 performance across benchmarks. Best results <strong>bolded</strong>; red deltas show absolute gain over base. Stage 1 trains on the full dataset. Stage 2 explores problem difficulty via three strategies: using only hard problems, mixing difficulties, or augmenting hard problems with partial solutions.</p>
      </div>
    </div>

    <!-- Insight 2: Sample Efficiency -->
    <div class="box">
      <h3 class="subtitle is-4" style="color: #8B0000;">
        💡 <b>Insight 2: RL improves sample efficiency.</b>
      </h3>
      <div class="has-text-centered">
        <img src="static/images/math_pass_k.png" style="max-width: 40%;">
      </div>
    </div>

    <!-- Insight 3: Planning -->
    <div class="box">
      <h3 class="subtitle is-4" style="color: #8B0000;">
        💡 <b>Insight 3: RL-tuned models are more flexible in plan execution—they can generate better plans than those provided by humans.</b>
      </h3>
      <div class="has-text-centered">
        <img src="static/images/model_comparison.png" style="max-width: 80%;">
      </div>
    </div>

    <!-- Insight 4: Knowledge -->
    <div class="box">
      <h3 class="subtitle is-4" style="color: #8B0000;">
        💡 <b>Insight 4: RL-tuned models can integrate external knowledge more effectively than base models.</b>
      </h3>
      <div class="has-text-centered">
        <img src="static/images/knowledge_model_comparison.png" style="max-width: 80%;">
      </div>
    </div>

    <!-- Insight 5: Subproblem Decomposition -->
    <div class="box">
      <h3 class="subtitle is-4" style="color: #8B0000;">
        💡 <b>Insight 5: RL-tuned models can solve the original problems (e.g., AIME24, AMC23, MATH500), but fail to solve the decomposed subproblems.</b>
      </h3>
      <div class="has-text-centered">
        <img src="static/images/task_decomposition_model_comparison.png" style="max-width: 80%;">
      </div>
    </div>

    <!-- Insight 6: Scaling with Difficulty -->
    <div class="box">
      <h3 class="subtitle is-4" style="color: #8B0000;">
        💡 <b>Insight 6: As problem difficulty increases, RL-tuned models benefit more from knowledge augmentation than high-level planning.</b>
      </h3>
      <div class="has-text-centered">
        <img src="static/images/base_rl_vanilla_planning_difficulty.png" style="max-width: 40%; margin-right: 1rem;">
        <img src="static/images/base_rl_vanilla_knowledge_difficulty.png" style="max-width: 40%;">
      </div>
    </div>

  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{wang2025accuracydissectingmathematicalreasoning,
        title={Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning}, 
        author={Jiayu Wang and Yifei Ming and Zixuan Ke and Caiming Xiong and Shafiq Joty and Aws Albarghouthi and Frederic Sala},
        year={2025},
        eprint={2506.04723},
        archivePrefix={arXiv},
        primaryClass={cs.AI},
        url={https://arxiv.org/abs/2506.04723}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
